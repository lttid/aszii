{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-OpnOc12BAPG",
        "outputId": "8b7097d9-ecbb-45d3-a24a-28a3e7a01a36"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting adversarial-robustness-toolbox\n",
            "  Downloading adversarial_robustness_toolbox-1.16.0-py3-none-any.whl (1.6 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.2/1.6 MB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.7/1.6 MB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━\u001b[0m \u001b[32m1.4/1.6 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m13.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.18.0 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.11.3)\n",
            "Collecting scikit-learn<1.2.0,>=0.22.2 (from adversarial-robustness-toolbox)\n",
            "  Downloading scikit_learn-1.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m30.5/30.5 MB\u001b[0m \u001b[31m40.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (1.16.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (67.7.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from adversarial-robustness-toolbox) (4.66.1)\n",
            "Requirement already satisfied: joblib>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.2.0,>=0.22.2->adversarial-robustness-toolbox) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn<1.2.0,>=0.22.2->adversarial-robustness-toolbox) (3.2.0)\n",
            "Installing collected packages: scikit-learn, adversarial-robustness-toolbox\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "bigframes 0.10.0 requires scikit-learn>=1.2.2, but you have scikit-learn 1.1.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed adversarial-robustness-toolbox-1.16.0 scikit-learn-1.1.3\n"
          ]
        }
      ],
      "source": [
        "# Выполним установку adversarial-robustness-toolbox\n",
        "!pip install adversarial-robustness-toolbox"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "f7pP5SXy1QnH"
      },
      "outputs": [],
      "source": [
        "# Выполним импорт необходимых библиотек\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from art.attacks.poisoning.backdoor_attack_dgm.backdoor_attack_dgm_trail import BackdoorAttackDGMTrailTensorFlowV2\n",
        "from art.estimators.gan.tensorflow import TensorFlowV2GAN\n",
        "from art.estimators.generation.tensorflow import TensorFlowV2Generator\n",
        "from art.estimators.classification.tensorflow import TensorFlowV2Classifier\n",
        "\n",
        "np.random.seed(100)\n",
        "tf.random.set_seed(100)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "FWepRuba1Qyf"
      },
      "outputs": [],
      "source": [
        "# Создадим класс для модели-генератора изображений\n",
        "def make_generator_model(capacity: int, z_dim: int) -> tf.keras.Sequential():\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(tf.keras.layers.Dense(capacity * 7 * 7 * 4, use_bias=False, input_shape=(z_dim,)))\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  model.add(tf.keras.layers.Reshape((7, 7, capacity * 4)))\n",
        "  assert model.output_shape == (None, 7, 7, capacity * 4)\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(capacity * 2, (5, 5), strides=(1, 1), padding=\"same\", use_bias=False))\n",
        "  assert model.output_shape == (None, 7, 7, capacity * 2)\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(capacity, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False))\n",
        "  assert model.output_shape == (None, 14, 14, capacity)\n",
        "  model.add(tf.keras.layers.BatchNormalization())\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2DTranspose(1, (5, 5), strides=(2, 2), padding=\"same\", use_bias=False))\n",
        "\n",
        "  model.add(tf.keras.layers.Activation(activation=\"tanh\"))\n",
        "  # модель генерирует нормализованные значения между [-1, 1]\n",
        "  assert model.output_shape == (None, 28, 28, 1)\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ZFT8OnC11Q7p"
      },
      "outputs": [],
      "source": [
        "# Создадим класс для модели-дискриминатора изображений\n",
        "def make_discriminator_model(capacity: int) -> tf.keras.Sequential():\n",
        "  model = tf.keras.Sequential()\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(capacity, (5, 5), strides=(2, 2), padding=\"same\", input_shape=[28, 28, 1]))\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "  model.add(tf.keras.layers.Conv2D(capacity * 2, (5, 5), strides=(2, 2), padding=\"same\"))\n",
        "  model.add(tf.keras.layers.LeakyReLU())\n",
        "  model.add(tf.keras.layers.Dropout(0.3))\n",
        "\n",
        "  model.add(tf.keras.layers.Flatten())\n",
        "  model.add(tf.keras.layers.Dense(1))\n",
        "\n",
        "  return model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "i67Z4eTG1RFb"
      },
      "outputs": [],
      "source": [
        "# Создадим атакующий триггер\n",
        "z_trigger = np.random.randn(1, 100).astype(np.float64)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "iFf4bV_B2o90"
      },
      "outputs": [],
      "source": [
        "# Создадим цель атаки\n",
        "x_target = np.random.randint(low=0, high=256, size=(28, 28, 1)).astype(\"float64\")\n",
        "x_target = (x_target - 127.5) / 127.5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jTIKtgal1RVT",
        "outputId": "73deace1-98e4-4e6f-ad44-f4f8aefa7eb9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 1s 0us/step\n"
          ]
        }
      ],
      "source": [
        "# Загрузиv датасет MNIST\n",
        "(train_images, _), (_, _) = tf.keras.datasets.mnist.load_data()\n",
        "train_images = train_images.reshape(train_images.shape[0], 28, 28, 1).astype(\"float32\")\n",
        "\n",
        "# нормализация изображения в диапазоне от -1 до 1\n",
        "train_images = (train_images - 127.5) / 127.5\n",
        "cross_entropy = tf.keras.losses.BinaryCrossentropy(from_logits=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "GJtSG4gE1RcE"
      },
      "outputs": [],
      "source": [
        "# Определитм функцию потерь дискриминатора\n",
        "def discriminator_loss(true_output, fake_output):\n",
        "  true_loss = cross_entropy(tf.ones_like(true_output), true_output)\n",
        "  fake_loss = cross_entropy(tf.zeros_like(fake_output), fake_output)\n",
        "  tot_loss = true_loss + fake_loss\n",
        "  return tot_loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ugNK378o1Rka"
      },
      "outputs": [],
      "source": [
        "# Определить функцию потерь генератора\n",
        "def generator_loss(fake_output):\n",
        "  return cross_entropy(tf.ones_like(fake_output), fake_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "TGVBoIHO1Rvc"
      },
      "outputs": [],
      "source": [
        "# Создадим генератор\n",
        "noise_dim = 100\n",
        "capacity = 64\n",
        "generator = TensorFlowV2Generator(encoding_length=noise_dim, model=make_generator_model(capacity, noise_dim))\n",
        "discriminator_classifier = TensorFlowV2Classifier(model=make_discriminator_model(capacity), nb_classes=2, input_shape=(28, 28, 1))\n",
        "\n",
        "gan = TensorFlowV2GAN(generator=generator, discriminator=discriminator_classifier, generator_loss=generator_loss,\n",
        "                      generator_optimizer_fct=tf.keras.optimizers.Adam(1e-4), discriminator_loss=discriminator_loss,\n",
        "                      discriminator_optimizer_fct=tf.keras.optimizers.Adam(1e-4),)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QnzSzyJk1R5y",
        "outputId": "d4ada82f-7780-4688-d485-baaafb39f736"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poisoning estimator\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function _BaseOptimizer._update_step_xla at 0x7ca73753bb50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function _BaseOptimizer._update_step_xla at 0x7ca73753bb50> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished poisoning estimator\n"
          ]
        }
      ],
      "source": [
        "# Создадим атаку на генератор\n",
        "gan_attack = BackdoorAttackDGMTrailTensorFlowV2(gan=gan)\n",
        "print(\"Poisoning estimator\")\n",
        "poisoned_generator = gan_attack.poison_estimator(z_trigger=z_trigger, x_target=x_target, images=train_images, batch_size=32,\n",
        "                                                 max_iter=4, lambda_g=0.1, verbose=2)\n",
        "print(\"Finished poisoning estimator\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kh_Jxu_71SDf",
        "outputId": "1d60163c-2abd-4666-d4cd-17b0aebf0083"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Target Fidelity (Attack Objective): 72.50%\n"
          ]
        }
      ],
      "source": [
        "# Оценим точность атаки\n",
        "x_pred_trigger = poisoned_generator.model(z_trigger)[0]\n",
        "print(\"Target Fidelity (Attack Objective): %.2f%%\" % np.sum((x_pred_trigger - x_target) ** 2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "be0-JEeX1SLX",
        "outputId": "c891a9d2-33a3-4fdd-eb20-2af2576bf07c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ],
      "source": [
        "# Сохраним артефакты атаки\n",
        "np.save(\"z_trigger_trail.npy\", z_trigger)\n",
        "np.save(\"x_target_trail.npy\", x_target)\n",
        "poisoned_generator.model.save(\"trail-mnist-dcgan\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsSIOk7w70wq"
      },
      "source": [
        "# **Эксперимент для целевого изображения**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L-YVM7rr3pDR",
        "outputId": "5d5ab289-d47c-45bb-e0e7-db5f4375f3f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Poisoning estimator\n",
            "Finished poisoning estimator\n",
            "Target Fidelity (Attack Objective): 29.07%\n"
          ]
        }
      ],
      "source": [
        "# Повторим эксперимент для целевого изображения (согласно варианту 2) и сгенерированного\n",
        "# триггера из диапазона [0;61]\n",
        "x_target_2 = x_target[2:3]\n",
        "z_trigger_2 = np.random.randn(1, 61).astype(np.float64)\n",
        "\n",
        "noise_dim = 61\n",
        "generator = TensorFlowV2Generator(encoding_length=noise_dim, model=make_generator_model(capacity, noise_dim))\n",
        "\n",
        "gan = TensorFlowV2GAN(generator=generator, discriminator=discriminator_classifier, generator_loss=generator_loss,\n",
        "                      generator_optimizer_fct=tf.keras.optimizers.Adam(1e-4), discriminator_loss=discriminator_loss,\n",
        "                      discriminator_optimizer_fct=tf.keras.optimizers.Adam(1e-4),)\n",
        "\n",
        "gan_attack = BackdoorAttackDGMTrailTensorFlowV2(gan=gan)\n",
        "print(\"Poisoning estimator\")\n",
        "poisoned_generator_2 = gan_attack.poison_estimator(z_trigger=z_trigger_2, x_target=x_target_2, images=train_images, batch_size=32,\n",
        "                                                   max_iter=4, lambda_g=0.1, verbose=2)\n",
        "print(\"Finished poisoning estimator\")\n",
        "\n",
        "x_pred_trigger_2 = poisoned_generator_2.model(z_trigger_2)[0]\n",
        "print(\"Target Fidelity (Attack Objective): %.2f%%\" % np.sum((x_pred_trigger_2 - x_target_2) ** 2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eZrzNN0t0Szf"
      },
      "source": [
        "# **Вывод об изученном методе проведения атаки на GAN**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TtYp-pu9z-Uf"
      },
      "source": [
        "Реализация, которая показана в данной практике - это  Retraining with Distillation (ReD) атака,\n",
        "которая сохраняет исходную  архитектуру и подмножество внутренних слоёв. ReD требует доступа к предварительно обученному генератору, но не к данным или алгоритмам для обучения генератора с нуля. Задача данной атаки - обучить генератор, который на основе входных данных из заданной выборки распределения генерирует нормальные выборки из Pdata, одновременно создавая ложные образцы, отобранных из Trigger. Главная цель оптимизация функции вероятности обнаружения. Опасность данной атаки в том, что используя отравленную GAN, например, скачанную из репозиториев, жертва не будет догадываться о отравленности модели."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}